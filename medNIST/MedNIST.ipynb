{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Installation**\n",
        "This code will use the dataset from MONAI github : https://github.com/Project-MONAI/MONAI-extra-test-data/\n",
        "\n",
        "Code below installs MedNIST.tar.gz file from MONAI github and unzips in a folder named MedNIST\n",
        "\n",
        "If you already have MedNIST dataset ready, assign the directory(str) to **`MedNIST_DATA_DIR`**\n",
        "\n",
        "It may take some time on colab for MedNIST folder to be visible"
      ],
      "metadata": {
        "id": "gur_fD4hbBOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget # this is for colab users since colab does not have wget package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ_Rtv8vcpvX",
        "outputId": "8cc8807b-187e-448a-99b0-82e03c525301"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=b4fa6bb1098877f1cd2e93a9cceee9e8de89b9d9b20a853c15edb3d538f75fe9\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "import wget\n",
        "import os\n",
        "\n",
        "MedNIST_DATA_DIR = None # change here\n",
        "\n",
        "if MedNIST_DATA_DIR!=None:\n",
        "  print(\"skip MedNIST dataset installation\")\n",
        "\n",
        "else:\n",
        "  if os.path.isfile('MedNIST.tar.gz'):\n",
        "    print(f\"skip pulling MedNIST dataset from MONAI github\")\n",
        "\n",
        "  else:\n",
        "    wget.download(\"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\")\n",
        "\n",
        "  print(\"unziping ...\")\n",
        "  if os.path.isfile(\"./MedNIST\"):\n",
        "    os.system(\"rm -rf MedNIST\")\n",
        "  \n",
        "  os.system(\"tar -xf MedNIST.tar.gz\")\n",
        "  MedNIST_DATA_DIR = f\"./MedNIST\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLiaA_tVcAju",
        "outputId": "65fec110-3172-45d1-b942-bf03c17c5355"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unziping ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages**\n",
        "Basically, the whole code uses **`torch`**  as main framework, however, there are several packages needed to preprocess the MONAI dataset\n",
        "\n",
        "**numpy** : basical arrray handling package useful in dealing with multi dimension array\n",
        "\n",
        "**PIL** : package used to read images and convert them into array\n",
        "\n",
        "**matplotlib** : most perferred package when displaying images in *python*\n",
        "\n",
        "**random** : package that provides useful functions when shuffling dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "R5GxuZgKVKqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BwgvklVdVDiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "from PIL import Image\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess parameters and constants**\n",
        "\n",
        "Preprocessing is the most important part of whole page.\n",
        "\n",
        "The final model will be a classifier, so there is one more thing to consider when preprocessing the dataset, other than normalization or shuffling.\n",
        "\n",
        "When dealing with this kind of multi-class classifier, it is important for trainset to contain **equal portion of data in each class**\n",
        "\n",
        "So, instead of mixing all data and spliting them, we will split data from each class by certain ratio which will be defined as **`TEST_RATIO`**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LhbxJXC1VWxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_RATIO = 0.2"
      ],
      "metadata": {
        "id": "uiHssCCjV2cN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then iterate through all classes to concatnate the long part in to trainset, and the other part to testset\n",
        "\n",
        "If dataset is saved in npy format, it will simply load it"
      ],
      "metadata": {
        "id": "pKwsiIMkoKrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"./X_train.npy\") and os.path.isfile(\"./Y_train\") and os.path.isfile(\"./X_test.npy\") and os.path.isfile(\"./Y_test.npy\"):\n",
        "  train_x = numpy.load('./X_train.npy')\n",
        "  train_y = numpy.load('./Y_train.npy')\n",
        "  valid_x = numpy.load('./X_test.npy')\n",
        "  valid_y = numpy.load('./Y_test.npy')\n",
        "\n",
        "else:\n",
        "  data_folder_list = [ name for name in os.listdir(f\"{MedNIST_DATA_DIR}\") if os.path.isdir(os.path.join(f\"{MedNIST_DATA_DIR}\", name)) ]\n",
        "  dataset_dict = dict()\n",
        "  for category,folder in enumerate(data_folder_list):\n",
        "      dataset_dict[f'{folder}'] = list()\n",
        "      for img in os.listdir(f'{MedNIST_DATA_DIR}/{folder}'):\n",
        "          img_array = numpy.asarray(Image.open(f'{MedNIST_DATA_DIR}/{folder}/{img}'))\n",
        "          dataset_dict[f'{folder}'].append([img_array,category])\n",
        "  tmp_test = list()\n",
        "  tmp_train = list()\n",
        "  \n",
        "  for key in dataset_dict.keys():\n",
        "      tmp_test.append(dataset_dict[key][:int(TEST_RATIO*len(dataset_dict[key]))])\n",
        "      tmp_train.append(dataset_dict[key][int(TEST_RATIO*len(dataset_dict[key])):])\n",
        "\n",
        "  train_x = list()\n",
        "  train_y = list()\n",
        "  valid_x = list()\n",
        "  valid_y = list()\n",
        "\n",
        "  for key in range(6):\n",
        "      for train_data_x, train_data_y in tmp_train[key]:\n",
        "          train_x.append([train_data_x])\n",
        "          train_y.append(train_data_y)\n",
        "\n",
        "      for test_data_x, test_data_y in tmp_test[key]:\n",
        "          valid_x.append([test_data_x])\n",
        "          valid_y.append(test_data_y)\n",
        "\n",
        "  train_x = numpy.asarray(train_x)\n",
        "  train_y = numpy.asarray(train_y)\n",
        "  valid_x = numpy.asarray(valid_x)\n",
        "  valid_y = numpy.asarray(valid_y)\n",
        "\n",
        "  train_x = train_x/255.0 # normalization on train_image\n",
        "  valid_x = valid_x/255.0 # normalization on test_image\n",
        "\n",
        "  train_y = numpy.ravel(train_y)\n",
        "  valid_y = numpy.ravel(valid_y)\n",
        "  \n",
        "  train_index = numpy.arange(len(train_x))\n",
        "  random.Random(6).shuffle(train_index)\n",
        "\n",
        "  train_x = train_x[train_index]\n",
        "  train_y = train_y[train_index]\n",
        "\n",
        "  test_index = numpy.arange(len(valid_x))\n",
        "  random.Random(7).shuffle(test_index)\n",
        "\n",
        "  valid_x = valid_x[test_index]\n",
        "  valid_y = valid_y[test_index]\n",
        "\n",
        "  numpy.save('./X_train.npy',train_x)\n",
        "  numpy.save('./Y_train.npy',train_y)\n",
        "  numpy.save('./X_test.npy',valid_x)\n",
        "  numpy.save('./Y_test.npy',valid_y)\n",
        "\n",
        "print(\"data preparation completed ..\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-bmRLGkVo-M",
        "outputId": "6f94a08f-cb68-4cd3-8f6a-30df5fb9740e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data preparation completed ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to Tensor**\n",
        "\n",
        "Pytorch Model only takes Tensor as input and label.\n",
        "So, numpy-format data must be converted to Tensor format.\n",
        "\n",
        "(**Inputs**)Image data should be in FloatTensor type and (**labels**)indicies should be in LongTensor type. "
      ],
      "metadata": {
        "id": "dNcTzIHJ5hz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.FloatTensor(train_x)\n",
        "valid_x = torch.FloatTensor(valid_x)\n",
        "train_y = torch.LongTensor(train_y)\n",
        "valid_y = torch.LongTensor(valid_y)"
      ],
      "metadata": {
        "id": "rtX-Brlkn-NI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Dataloader**\n",
        "\n",
        "Dataloader is the most important characteristic of Pytorch.\n",
        "\n",
        "Before assigning the tensor input and label, they must be packed in TensorDataset\n",
        "\n",
        "Batch size is determines the size of minibatch per iteration.\n",
        "\n",
        "Assign the size of minibatch in **`BATCH_SIZE`**.\n",
        "\n",
        "When training, it will enumerate dataloader, which will automatically schedule minibatch per iteration"
      ],
      "metadata": {
        "id": "yAkiF-JGWJ_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "metadata": {
        "id": "8Iuv7zuzWMsw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torch.utils.data.TensorDataset(train_x, train_y)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "valid_set = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "5W9SUGmrWawo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Device**\n",
        "When using Pytroch, user have to select device, or it will use cpu by default\n",
        "\n",
        "If using multi-gpu. select one gpu-id and insert after `cuda:`"
      ],
      "metadata": {
        "id": "4d8LoKHwWx7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "B482p8blWuW3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Config Model**\n",
        "Model is a light cnn model\n",
        "\n",
        "**Note that there is no activation function at the last layer**\n",
        "\n",
        "`CrossEntropyLoss` will be used later, and pytorch cross-entropy-loss calculates softmax-logit value internaly. "
      ],
      "metadata": {
        "id": "av4l7714WzX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv2d_Model, self).__init__()\n",
        "        self.convolution_layer1 = nn.Conv2d(1,6,3)\n",
        "        self.convolution_layer2 = nn.Conv2d(6,12,3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dense_layer1 = nn.Linear(14*14*12, 256)\n",
        "        self.dense_layer2 = nn.Linear(256, 32)\n",
        "        self.dense_layer3 = nn.Linear(32, 6)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.convolution_layer1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.convolution_layer2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.dense_layer1(x))\n",
        "        x = F.relu(self.dense_layer2(x))\n",
        "        x = self.dense_layer3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e4w-gP7oW5mE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Model object**\n",
        "Unlike Tensorflow, Pytorch does not automatically load model on gpu"
      ],
      "metadata": {
        "id": "oG1tTg42Xxm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv2d_Model()\n",
        "if USE_CUDA: model.to(device)"
      ],
      "metadata": {
        "id": "sLGpgqyxXl0P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Loss Function and Optimizer**\n",
        "After creating model object, assign its parameters to optimizer.\n",
        "\n",
        "This way, model parameter updating equation is determined."
      ],
      "metadata": {
        "id": "IKBIbCwgXBhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lYfU3jfcXhcg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameters for training session**\n",
        "Pytorch does not have simple methods to train model, however, dataloader makes it simple.\n",
        "\n",
        "As mentioned before, Pytorch's training session is usually done by enumerating through trains-dataloader. Then, by every loop, loss will be calculated and gradient will be updated within given minibatch.\n",
        "\n",
        "## **Metric**\n",
        "However, it would be more convienient if we could monitor it.\n",
        "The code below counts the progress of training and the loss for each epoch.\n",
        "`print('\\r',f\"training {train_idx+1}/{len(train_loader)}, train_loss: {train_loss:0.4f}\",end=\" \")`\n",
        "\n",
        "\n",
        "Moreover, to check if the model is trained correctly, we can test it on validation-set and also print it.\n",
        "\n",
        "`print('\\r',f\"validing {val_idx+1}/{len(valid_loader)}, val_loss:{tot_val_loss:0.4f}, val_acc: {acc:0.4}%\", end=\" \")`\n",
        "\n",
        "\n",
        "\n",
        "## **Early Stopping**\n",
        "While there can be many type of custom training session, **early stopping** is one of the most frequently used technique.\n",
        "\n",
        "So, early stopping will be used in the code below. It can be accomplished by defining few parameters.\n",
        "\n",
        "\n",
        "## **Parameter**\n",
        "**`EPOCH`**: The total number of loop of model training\n",
        "\n",
        "**`CURRENT_PATIENCE`**: Current patience count. After every epoch, if validation loss did not get smaller than before, patience count is increased.\n",
        "\n",
        "**`STANDARD_VAL_LOSS`**: The initial validation loss. If the validation loss is smaller than this, it will be replaced with latest validation loss.\n",
        "\n",
        "**`PATIENCE_LIMIT`**: The max patientce count. If patient count reaches this point, training session, it will halt the training session. "
      ],
      "metadata": {
        "id": "wrirMyqTYNJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 100\n",
        "PATIENCE_LIMIT = 8\n",
        "CURRENT_PAIENCE = 0\n",
        "STANDARD_VAL_LOSS = 10**9"
      ],
      "metadata": {
        "id": "AYT7ofMJYUWb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Session**"
      ],
      "metadata": {
        "id": "9qDOYMOLX61E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCH):\n",
        "  print(f\"---------------Epoch : {epoch+1}/{EPOCH}--------------------\")\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for train_idx, data in enumerate(train_loader, 0):\n",
        "    optimizer.zero_grad()\n",
        "    print('\\r',f\"training {train_idx+1}/{len(train_loader)}, train_loss: {train_loss:0.4f}\",end=\" \")\n",
        "    inputs, labels = data\n",
        "\n",
        "    outputs = model(inputs.to(device))\n",
        "    loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  print('')\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  tot_val_loss = 0.0\n",
        "  acc = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    '''\n",
        "    Omitting this no_grad condition will not affect the model training, since we will update the loss calcuated in validation-set.\n",
        "    However, it is conventional to use torch.no_grad in model validation.\n",
        "    '''\n",
        "    for val_idx, val_data in enumerate(valid_loader, 0):\n",
        "      print('\\r',f\"validing {val_idx+1}/{len(valid_loader)}, val_loss:{tot_val_loss:0.4f}, val_acc: {acc:0.4}%\", end=\" \")\n",
        "        \n",
        "      val_inputs, val_label = val_data\n",
        "      val_output = model(val_inputs.to(device))\n",
        "      val_loss = criterion(val_output, val_label.to(device))\n",
        "        \n",
        "      prediction = torch.argmax(val_output,1)\n",
        "      tot_val_loss += val_loss.item() \n",
        "\n",
        "      total += val_label.size(0)\n",
        "      correct += (prediction == val_label.to(device)).sum().item()\n",
        "      acc = 100.0*correct/total\n",
        "    print('')\n",
        "    print('\\n')\n",
        "  \n",
        "  if PATIENCE_LIMIT > CURRENT_PAIENCE:\n",
        "\n",
        "    if val_loss < STANDARD_VAL_LOSS:\n",
        "      \n",
        "      STANDARD_VAL_LOSS = val_loss\n",
        "      best_epoch = epoch+1\n",
        "      best_model = model\n",
        "      CURRENT_PAIENCE = 0\n",
        "\n",
        "    else:\n",
        "      CURRENT_PAIENCE += 1\n",
        "  else:break\n",
        "\n",
        "print('')\n",
        "print('\\n')\n",
        "\n",
        "if (epoch+1) != EPOCH:\n",
        "  print(\"Early Stopping ...\")\n",
        "\n",
        "if os.path.isfile('./2D_CNN_model_parameter'):\n",
        "  os.remove('./2D_CNN_model_parameter')\n",
        "\n",
        "torch.save(best_model.state_dict(), './2D_CNN_model_parameter')\n",
        "print(\"================================================\")\n",
        "print(f\"model parameters from epoch {best_epoch} saved!\")\n",
        "print(\"================================================\")\n",
        "print(\"\\n\")\n",
        "print(\".. model train finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCBEy9zCX6Cs",
        "outputId": "4a6e724e-d27b-4564-9bb4-47a40893a9ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------Epoch : 1/100--------------------\n",
            " training 47/47, train_loss: 21.7071 \n",
            " validing 12/12, val_loss:0.4026, val_acc: 98.97% \n",
            "\n",
            "\n",
            "---------------Epoch : 2/100--------------------\n",
            " training 47/47, train_loss: 0.6671 \n",
            " validing 12/12, val_loss:0.0763, val_acc: 99.77% \n",
            "\n",
            "\n",
            "---------------Epoch : 3/100--------------------\n",
            " training 47/47, train_loss: 0.2747 \n",
            " validing 12/12, val_loss:0.0504, val_acc: 99.8% \n",
            "\n",
            "\n",
            "---------------Epoch : 4/100--------------------\n",
            " training 47/47, train_loss: 0.2023 \n",
            " validing 12/12, val_loss:0.0441, val_acc: 99.85% \n",
            "\n",
            "\n",
            "---------------Epoch : 5/100--------------------\n",
            " training 47/47, train_loss: 0.1378 \n",
            " validing 12/12, val_loss:0.0366, val_acc: 99.87% \n",
            "\n",
            "\n",
            "---------------Epoch : 6/100--------------------\n",
            " training 47/47, train_loss: 0.0966 \n",
            " validing 12/12, val_loss:0.0441, val_acc: 99.87% \n",
            "\n",
            "\n",
            "---------------Epoch : 7/100--------------------\n",
            " training 47/47, train_loss: 0.0989 \n",
            " validing 12/12, val_loss:0.0320, val_acc: 99.9% \n",
            "\n",
            "\n",
            "---------------Epoch : 8/100--------------------\n",
            " training 47/47, train_loss: 0.0791 \n",
            " validing 12/12, val_loss:0.0317, val_acc: 99.92% \n",
            "\n",
            "\n",
            "---------------Epoch : 9/100--------------------\n",
            " training 47/47, train_loss: 0.1585 \n",
            " validing 12/12, val_loss:0.0296, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 10/100--------------------\n",
            " training 47/47, train_loss: 0.0867 \n",
            " validing 12/12, val_loss:0.0337, val_acc: 99.89% \n",
            "\n",
            "\n",
            "---------------Epoch : 11/100--------------------\n",
            " training 47/47, train_loss: 0.0606 \n",
            " validing 12/12, val_loss:0.0298, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 12/100--------------------\n",
            " training 47/47, train_loss: 0.0667 \n",
            " validing 12/12, val_loss:0.0392, val_acc: 99.88% \n",
            "\n",
            "\n",
            "---------------Epoch : 13/100--------------------\n",
            " training 47/47, train_loss: 0.0560 \n",
            " validing 12/12, val_loss:0.0265, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 14/100--------------------\n",
            " training 47/47, train_loss: 0.0198 \n",
            " validing 12/12, val_loss:0.0364, val_acc: 99.88% \n",
            "\n",
            "\n",
            "---------------Epoch : 15/100--------------------\n",
            " training 47/47, train_loss: 0.0221 \n",
            " validing 12/12, val_loss:0.0350, val_acc: 99.88% \n",
            "\n",
            "\n",
            "---------------Epoch : 16/100--------------------\n",
            " training 47/47, train_loss: 0.0565 \n",
            " validing 12/12, val_loss:0.0331, val_acc: 99.95% \n",
            "\n",
            "\n",
            "---------------Epoch : 17/100--------------------\n",
            " training 47/47, train_loss: 0.0243 \n",
            " validing 12/12, val_loss:0.0393, val_acc: 99.88% \n",
            "\n",
            "\n",
            "---------------Epoch : 18/100--------------------\n",
            " training 47/47, train_loss: 0.0129 \n",
            " validing 12/12, val_loss:0.0438, val_acc: 99.92% \n",
            "\n",
            "\n",
            "---------------Epoch : 19/100--------------------\n",
            " training 47/47, train_loss: 0.0166 \n",
            " validing 12/12, val_loss:0.0437, val_acc: 99.89% \n",
            "\n",
            "\n",
            "---------------Epoch : 20/100--------------------\n",
            " training 47/47, train_loss: 0.0055 \n",
            " validing 12/12, val_loss:0.0428, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 21/100--------------------\n",
            " training 47/47, train_loss: 0.0030 \n",
            " validing 12/12, val_loss:0.0465, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 22/100--------------------\n",
            " training 47/47, train_loss: 0.0011 \n",
            " validing 12/12, val_loss:0.0433, val_acc: 99.94% \n",
            "\n",
            "\n",
            "---------------Epoch : 23/100--------------------\n",
            " training 47/47, train_loss: 0.0013 \n",
            " validing 12/12, val_loss:0.0415, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 24/100--------------------\n",
            " training 47/47, train_loss: 0.0005 \n",
            " validing 12/12, val_loss:0.0429, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 25/100--------------------\n",
            " training 47/47, train_loss: 0.0004 \n",
            " validing 12/12, val_loss:0.0430, val_acc: 99.93% \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Early Stopping ...\n",
            "================================================\n",
            "model parameters from epoch 16 saved!\n",
            "================================================\n",
            "\n",
            "\n",
            ".. model train finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dba0RZf-Pv2E"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Installation**\n",
        "This code will use the dataset from MONAI github : https://github.com/Project-MONAI/MONAI-extra-test-data/\n",
        "\n",
        "Code below installs MedNIST.tar.gz file from MONAI github and unzips in a folder named MedNIST\n",
        "\n",
        "If you already have MedNIST dataset ready, assign the directory(str) to **`MedNIST_DATA_DIR`**\n",
        "\n",
        "It may take some time on colab for MedNIST folder to be visible"
      ],
      "metadata": {
        "id": "gur_fD4hbBOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget # this is for colab users since colab does not have wget package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ_Rtv8vcpvX",
        "outputId": "8626dd41-1046-4ba5-8dcd-214b671c7d2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.9/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "import wget\n",
        "import os\n",
        "\n",
        "MedNIST_DATA_DIR = None # change here\n",
        "\n",
        "if MedNIST_DATA_DIR!=None:\n",
        "  print(\"skip MedNIST dataset installation\")\n",
        "\n",
        "else:\n",
        "  if os.path.isfile('MedNIST.tar.gz'):\n",
        "    print(f\"skip pulling MedNIST dataset from MONAI github\")\n",
        "\n",
        "  else:\n",
        "    wget.download(\"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\")\n",
        "\n",
        "  print(\"unziping ...\")\n",
        "  if os.path.isfile(\"./MedNIST\"):\n",
        "    os.system(\"rm -rf MedNIST\")\n",
        "  \n",
        "  os.system(\"tar -xf MedNIST.tar.gz\")\n",
        "  MedNIST_DATA_DIR = f\"./MedNIST\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLiaA_tVcAju",
        "outputId": "06f5daaf-7781-4246-f7d0-596e1d8feb1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skip pulling MedNIST dataset from MONAI github\n",
            "unziping ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages**\n",
        "Basically, the whole code uses **`torch`**  as main framework, however, there are several packages needed to preprocess the MONAI dataset\n",
        "\n",
        "**numpy** : basical arrray handling package useful in dealing with multi dimension array\n",
        "\n",
        "**PIL** : package used to read images and convert them into array\n",
        "\n",
        "**matplotlib** : most perferred package when displaying images in *python*\n",
        "\n",
        "**random** : package that provides useful functions when shuffling dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "R5GxuZgKVKqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BwgvklVdVDiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "from PIL import Image\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess parameters and constants**\n",
        "\n",
        "Preprocessing is the most important part of whole page.\n",
        "\n",
        "The final model will be a classifier, so there is one more thing to consider when preprocessing the dataset, other than normalization or shuffling.\n",
        "\n",
        "When dealing with this kind of multi-class classifier, it is important for trainset to contain **equal portion of data in each class**\n",
        "\n",
        "So, instead of mixing all data and spliting them, we will split data from each class by certain ratio which will be defined as **`TEST_RATIO`**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LhbxJXC1VWxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_RATIO = 0.2"
      ],
      "metadata": {
        "id": "uiHssCCjV2cN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then iterate through all classes to concatnate the long part in to trainset, and the other part to testset\n",
        "\n",
        "If dataset is saved in npy format, it will simply load it"
      ],
      "metadata": {
        "id": "pKwsiIMkoKrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"./X_train.npy\") and os.path.isfile(\"./Y_train\") and os.path.isfile(\"./X_test.npy\") and os.path.isfile(\"./Y_test.npy\"):\n",
        "  train_x = numpy.load('./X_train.npy')\n",
        "  train_y = numpy.load('./Y_train.npy')\n",
        "  valid_x = numpy.load('./X_test.npy')\n",
        "  valid_y = numpy.load('./Y_test.npy')\n",
        "\n",
        "else:\n",
        "  data_folder_list = [ name for name in os.listdir(f\"{MedNIST_DATA_DIR}\") if os.path.isdir(os.path.join(f\"{MedNIST_DATA_DIR}\", name)) ]\n",
        "  dataset_dict = dict()\n",
        "  for category,folder in enumerate(data_folder_list):\n",
        "      dataset_dict[f'{folder}'] = list()\n",
        "      for img in os.listdir(f'{MedNIST_DATA_DIR}/{folder}'):\n",
        "          img_array = numpy.asarray(Image.open(f'{MedNIST_DATA_DIR}/{folder}/{img}'))\n",
        "          dataset_dict[f'{folder}'].append([img_array,category])\n",
        "  tmp_test = list()\n",
        "  tmp_train = list()\n",
        "  \n",
        "  for key in dataset_dict.keys():\n",
        "      tmp_test.append(dataset_dict[key][:int(TEST_RATIO*len(dataset_dict[key]))])\n",
        "      tmp_train.append(dataset_dict[key][int(TEST_RATIO*len(dataset_dict[key])):])\n",
        "\n",
        "  train_x = list()\n",
        "  train_y = list()\n",
        "  valid_x = list()\n",
        "  valid_y = list()\n",
        "\n",
        "  for key in range(6):\n",
        "      for train_data_x, train_data_y in tmp_train[key]:\n",
        "          train_x.append([train_data_x])\n",
        "          train_y.append(train_data_y)\n",
        "\n",
        "      for test_data_x, test_data_y in tmp_test[key]:\n",
        "          valid_x.append([test_data_x])\n",
        "          valid_y.append(test_data_y)\n",
        "\n",
        "  train_x = numpy.asarray(train_x)\n",
        "  train_y = numpy.asarray(train_y)\n",
        "  valid_x = numpy.asarray(valid_x)\n",
        "  valid_y = numpy.asarray(valid_y)\n",
        "\n",
        "  train_x = train_x/255.0 # normalization on train_image\n",
        "  valid_x = valid_x/255.0 # normalization on test_image\n",
        "\n",
        "  train_y = numpy.ravel(train_y)\n",
        "  valid_y = numpy.ravel(valid_y)\n",
        "  \n",
        "  train_index = numpy.arange(len(train_x))\n",
        "  random.Random(6).shuffle(train_index)\n",
        "\n",
        "  train_x = train_x[train_index]\n",
        "  train_y = train_y[train_index]\n",
        "\n",
        "  test_index = numpy.arange(len(valid_x))\n",
        "  random.Random(7).shuffle(test_index)\n",
        "\n",
        "  valid_x = valid_x[test_index]\n",
        "  valid_y = valid_y[test_index]\n",
        "\n",
        "  numpy.save('./X_train.npy',train_x)\n",
        "  numpy.save('./Y_train.npy',train_y)\n",
        "  numpy.save('./X_test.npy',valid_x)\n",
        "  numpy.save('./Y_test.npy',valid_y)\n",
        "\n",
        "print(\"data preparation completed ..\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-bmRLGkVo-M",
        "outputId": "81615d1d-6dea-4f6c-b29d-7b81616ec244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data preparation completed ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to Tensor**\n",
        "\n",
        "Pytorch Model only takes Tensor as input and label.\n",
        "So, numpy-format data must be converted to Tensor format.\n",
        "\n",
        "(**Inputs**)Image data should be in FloatTensor type and (**labels**)indicies should be in LongTensor type. "
      ],
      "metadata": {
        "id": "dNcTzIHJ5hz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.FloatTensor(train_x)\n",
        "valid_x = torch.FloatTensor(valid_x)\n",
        "train_y = torch.LongTensor(train_y)\n",
        "valid_y = torch.LongTensor(valid_y)"
      ],
      "metadata": {
        "id": "rtX-Brlkn-NI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Dataloader**\n",
        "\n",
        "Dataloader is the most important characteristic of Pytorch.\n",
        "\n",
        "Before assigning the tensor input and label, they must be packed in TensorDataset\n",
        "\n",
        "Batch size is determines the size of minibatch per iteration.\n",
        "\n",
        "Assign the size of minibatch in **`BATCH_SIZE`**.\n",
        "\n",
        "When training, it will enumerate dataloader, which will automatically schedule minibatch per iteration"
      ],
      "metadata": {
        "id": "yAkiF-JGWJ_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024"
      ],
      "metadata": {
        "id": "8Iuv7zuzWMsw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torch.utils.data.TensorDataset(train_x, train_y)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "valid_set = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "5W9SUGmrWawo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Device**\n",
        "When using Pytroch, user have to select device, or it will use cpu by default\n",
        "\n",
        "If using multi-gpu. select one gpu-id and insert after `cuda:`"
      ],
      "metadata": {
        "id": "4d8LoKHwWx7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "B482p8blWuW3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Config Model**\n",
        "Model is a light cnn model\n",
        "\n",
        "**Note that there is no activation function at the last layer**\n",
        "\n",
        "`CrossEntropyLoss` will be used later, and pytorch cross-entropy-loss calculates softmax-logit value internaly. "
      ],
      "metadata": {
        "id": "av4l7714WzX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv2d_Model, self).__init__()\n",
        "        self.convolution_layer1 = nn.Conv2d(1,6,3)\n",
        "        self.convolution_layer2 = nn.Conv2d(6,12,3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dense_layer1 = nn.Linear(14*14*12, 256)\n",
        "        self.dense_layer2 = nn.Linear(256, 32)\n",
        "        self.dense_layer3 = nn.Linear(32, 6)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.convolution_layer1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.convolution_layer2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.dense_layer1(x))\n",
        "        x = F.relu(self.dense_layer2(x))\n",
        "        x = self.dense_layer3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e4w-gP7oW5mE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Model object**"
      ],
      "metadata": {
        "id": "oG1tTg42Xxm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv2d_Model()\n",
        "if USE_CUDA: model.cuda()"
      ],
      "metadata": {
        "id": "sLGpgqyxXl0P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Loss Function and Optimizer**"
      ],
      "metadata": {
        "id": "IKBIbCwgXBhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lYfU3jfcXhcg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameters for training session**"
      ],
      "metadata": {
        "id": "wrirMyqTYNJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 100\n",
        "PATIENCE_LIMIT = 8\n",
        "CURRENT_PAIENCE = 0\n",
        "STANDARD_VAL_LOSS = 10**9"
      ],
      "metadata": {
        "id": "AYT7ofMJYUWb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Session**"
      ],
      "metadata": {
        "id": "9qDOYMOLX61E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCH):\n",
        "  print(f\"---------------Epoch : {epoch+1}/{EPOCH}--------------------\")\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for train_idx, data in enumerate(train_loader, 0):\n",
        "    optimizer.zero_grad()\n",
        "    print('\\r',f\"training {train_idx+1}/{len(train_loader)}, train_loss: {train_loss:0.4f}\",end=\" \")\n",
        "    inputs, labels = data\n",
        "\n",
        "    outputs = model(inputs.to(device))\n",
        "    loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  print('')\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  tot_val_loss = 0.0\n",
        "  acc = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for val_idx, val_data in enumerate(valid_loader, 0):\n",
        "      print('\\r',f\"validing {val_idx+1}/{len(valid_loader)}, val_loss:{tot_val_loss:0.4f}, val_acc: {acc:0.4}%\", end=\" \")\n",
        "        \n",
        "      val_inputs, val_label = val_data\n",
        "      val_output = model(val_inputs.to(device))\n",
        "      val_loss = criterion(val_output, val_label.to(device))\n",
        "        \n",
        "      prediction = torch.argmax(val_output,1)\n",
        "      tot_val_loss += val_loss.item() \n",
        "\n",
        "      total += val_label.size(0)\n",
        "      correct += (prediction == val_label.to(device)).sum().item()\n",
        "      acc = 100.0*correct/total\n",
        "    print('')\n",
        "    print('\\n')\n",
        "  \n",
        "  if PATIENCE_LIMIT > CURRENT_PAIENCE:\n",
        "\n",
        "    if val_loss < STANDARD_VAL_LOSS:\n",
        "      \n",
        "      STANDARD_VAL_LOSS = val_loss\n",
        "      best_epoch = epoch+1\n",
        "      best_model = model\n",
        "      CURRENT_PAIENCE = 0\n",
        "\n",
        "    else:\n",
        "      CURRENT_PAIENCE += 1\n",
        "  else:break\n",
        "\n",
        "print('')\n",
        "print('\\n')\n",
        "\n",
        "if (epoch+1) != EPOCH:\n",
        "  print(\"Early Stopping ...\")\n",
        "\n",
        "if os.path.isfile('./2D_CNN_model_parameter'):\n",
        "  os.remove('./2D_CNN_model_parameter')\n",
        "\n",
        "torch.save(best_model.state_dict(), './2D_CNN_model_parameter')\n",
        "print(\"================================================\")\n",
        "print(f\"model parameters from epoch {best_epoch} saved!\")\n",
        "print(\"================================================\")\n",
        "print(\"\\n\")\n",
        "print(\".. model train finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCBEy9zCX6Cs",
        "outputId": "d3755dd1-4c5f-4c15-9261-3989546cc2e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------Epoch : 1/100--------------------\n",
            " training 47/47, train_loss: 26.3168 \n",
            " validing 12/12, val_loss:0.7909, val_acc: 98.03% \n",
            "\n",
            "\n",
            "---------------Epoch : 2/100--------------------\n",
            " training 47/47, train_loss: 2.0186 \n",
            " validing 12/12, val_loss:0.2374, val_acc: 99.41% \n",
            "\n",
            "\n",
            "---------------Epoch : 3/100--------------------\n",
            " training 47/47, train_loss: 0.7114 \n",
            " validing 12/12, val_loss:0.0947, val_acc: 99.76% \n",
            "\n",
            "\n",
            "---------------Epoch : 4/100--------------------\n",
            " training 47/47, train_loss: 0.3413 \n",
            " validing 12/12, val_loss:0.0492, val_acc: 99.86% \n",
            "\n",
            "\n",
            "---------------Epoch : 5/100--------------------\n",
            " training 47/47, train_loss: 0.1815 \n",
            " validing 12/12, val_loss:0.0283, val_acc: 99.9% \n",
            "\n",
            "\n",
            "---------------Epoch : 6/100--------------------\n",
            " training 47/47, train_loss: 0.0919 \n",
            " validing 12/12, val_loss:0.0253, val_acc: 99.91% \n",
            "\n",
            "\n",
            "---------------Epoch : 7/100--------------------\n",
            " training 47/47, train_loss: 0.0653 \n",
            " validing 12/12, val_loss:0.0214, val_acc: 99.92% \n",
            "\n",
            "\n",
            "---------------Epoch : 8/100--------------------\n",
            " training 47/47, train_loss: 0.0562 \n",
            " validing 12/12, val_loss:0.0186, val_acc: 99.93% \n",
            "\n",
            "\n",
            "---------------Epoch : 9/100--------------------\n",
            " training 47/47, train_loss: 0.0405 \n",
            " validing 12/12, val_loss:0.0317, val_acc: 99.9% \n",
            "\n",
            "\n",
            "---------------Epoch : 10/100--------------------\n",
            " training 47/47, train_loss: 0.0598 \n",
            " validing 12/12, val_loss:0.0808, val_acc: 99.78% \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Early Stopping ...\n",
            "================================================\n",
            "model parameters from epoch 4 saved!\n",
            "================================================\n",
            "\n",
            "\n",
            ".. model train finished\n"
          ]
        }
      ]
    }
  ]
}